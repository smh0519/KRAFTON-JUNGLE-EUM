"use client";

import { useEffect, useRef, useCallback, useState } from "react";

const WS_BASE_URL = process.env.NEXT_PUBLIC_VOICE_WS_URL || 'ws://localhost:8080/ws/audio';
const SAMPLE_RATE = Number(process.env.NEXT_PUBLIC_AUDIO_SAMPLE_RATE) || 16000;

export type ConnectionStatus = 'disconnected' | 'connecting' | 'handshaking' | 'ready' | 'error';
export type TargetLanguage = 'ko' | 'en' | 'ja' | 'zh';

export const SUPPORTED_LANGUAGES: { code: TargetLanguage; name: string; flag: string }[] = [
    { code: 'ko', name: 'ÌïúÍµ≠Ïñ¥', flag: 'üá∞üá∑' },
    { code: 'en', name: 'English', flag: 'üá∫üá∏' },
    { code: 'ja', name: 'Êó•Êú¨Ë™û', flag: 'üáØüáµ' },
    { code: 'zh', name: '‰∏≠Êñá', flag: 'üá®üá≥' },
];

interface HandshakeResponse {
    status: 'ready' | 'error';
    session_id: string;
    mode: 'ai' | 'echo';
    message?: string;
}

interface TranscriptMessage {
    type: 'transcript';
    participantId?: string;
    text: string;
    original: string;
    translated: string;
    isFinal: boolean;
}

export interface TranscriptData {
    participantId?: string;
    original: string;
    translated: string;
    isFinal: boolean;
}

interface UseAudioWebSocketConfig {
    sampleRate?: number;
    channels?: number;
    bitsPerSample?: number;
    targetLanguage?: TargetLanguage;
    participantId?: string;
    onTranscript?: (data: TranscriptData) => void;
    onAudio?: (audioData: ArrayBuffer, sampleRate: number, participantId?: string) => void;
    onStatusChange?: (status: ConnectionStatus) => void;
    onError?: (error: Error) => void;
    enabled?: boolean;
}

// 12Î∞îÏù¥Ìä∏ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ìó§Îçî ÏÉùÏÑ± (Little Endian)
function createMetadataHeader(sampleRate: number, channels: number, bitsPerSample: number): ArrayBuffer {
    const buffer = new ArrayBuffer(12);
    const view = new DataView(buffer);
    view.setUint32(0, sampleRate, true);    // Little Endian
    view.setUint16(4, channels, true);
    view.setUint16(6, bitsPerSample, true);
    view.setUint32(8, 0, true);             // Reserved
    return buffer;
}

export function useAudioWebSocket({
    sampleRate = SAMPLE_RATE,
    channels = 1,
    bitsPerSample = 16,
    targetLanguage = 'en',
    participantId,
    onTranscript,
    onAudio,
    onStatusChange,
    onError,
    enabled = false,
}: UseAudioWebSocketConfig = {}) {
    const wsRef = useRef<WebSocket | null>(null);
    const [status, setStatus] = useState<ConnectionStatus>('disconnected');
    const [sessionId, setSessionId] = useState<string | null>(null);
    const [mode, setMode] = useState<'ai' | 'echo' | null>(null);
    const reconnectTimeoutRef = useRef<NodeJS.Timeout | null>(null);
    const isHandshakeCompleteRef = useRef(false);
    const isMountedRef = useRef(false);
    const enabledRef = useRef(enabled);
    const targetLanguageRef = useRef(targetLanguage);
    const participantIdRef = useRef(participantId);

    // ÏΩúÎ∞± refs (ÏµúÏã† ÏΩúÎ∞± Ïú†ÏßÄ - Îß§ Î†åÎçîÎßàÎã§ ÏóÖÎç∞Ïù¥Ìä∏)
    const onTranscriptRef = useRef(onTranscript);
    const onAudioRef = useRef(onAudio);
    const onErrorRef = useRef(onError);
    onTranscriptRef.current = onTranscript;
    onAudioRef.current = onAudio;
    onErrorRef.current = onError;

    // enabled Í∞íÏùÑ refÏóê ÎèôÍ∏∞Ìôî
    useEffect(() => {
        enabledRef.current = enabled;
    }, [enabled]);

    // targetLanguage Í∞íÏùÑ refÏóê ÎèôÍ∏∞Ìôî
    useEffect(() => {
        targetLanguageRef.current = targetLanguage;
    }, [targetLanguage]);

    // participantId Í∞íÏùÑ refÏóê ÎèôÍ∏∞Ìôî
    useEffect(() => {
        participantIdRef.current = participantId;
    }, [participantId]);

    const updateStatus = useCallback((newStatus: ConnectionStatus) => {
        if (!isMountedRef.current) return;
        setStatus(newStatus);
        onStatusChange?.(newStatus);
    }, [onStatusChange]);

    const connect = useCallback(() => {
        console.log("[AudioWS] connect() called, enabled:", enabledRef.current);
        if (!enabledRef.current) return;

        // Ïù¥ÎØ∏ Ïó∞Í≤∞Ï§ëÏù¥Í±∞ÎÇò Ïó∞Í≤∞Îêú ÏÉÅÌÉúÎ©¥ Î¨¥Ïãú
        if (wsRef.current?.readyState === WebSocket.CONNECTING ||
            wsRef.current?.readyState === WebSocket.OPEN) {
            console.log("[AudioWS] Already connecting/connected, skipping");
            return;
        }

        // Ïñ∏Ïñ¥ Î∞è participantId ÌååÎùºÎØ∏ÌÑ∞ Ìè¨Ìï®Ìïú URL ÏÉùÏÑ±
        let wsUrl = `${WS_BASE_URL}?lang=${targetLanguageRef.current}`;
        if (participantIdRef.current) {
            wsUrl += `&participantId=${encodeURIComponent(participantIdRef.current)}`;
        }
        console.log("[AudioWS] Connecting to:", wsUrl, "with language:", targetLanguageRef.current, "participantId:", participantIdRef.current);
        updateStatus('connecting');
        isHandshakeCompleteRef.current = false;

        const ws = new WebSocket(wsUrl);
        ws.binaryType = 'arraybuffer';
        wsRef.current = ws;

        ws.onopen = () => {
            if (!isMountedRef.current) {
                ws.close();
                return;
            }
            console.log("[AudioWS] WebSocket opened, sending handshake...");
            updateStatus('handshaking');

            // Ìï∏ÎìúÏÖ∞Ïù¥ÌÅ¨: 12Î∞îÏù¥Ìä∏ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÑÏÜ°
            const metadata = createMetadataHeader(sampleRate, channels, bitsPerSample);
            ws.send(metadata);
        };

        ws.onmessage = (event) => {
            if (!isMountedRef.current) return;

            // Ìï∏ÎìúÏÖ∞Ïù¥ÌÅ¨ ÏùëÎãµ (JSON)
            if (!isHandshakeCompleteRef.current && typeof event.data === 'string') {
                try {
                    const response: HandshakeResponse = JSON.parse(event.data);
                    if (response.status === 'ready') {
                        console.log("[AudioWS] Handshake complete:", response);
                        isHandshakeCompleteRef.current = true;
                        setSessionId(response.session_id);
                        setMode(response.mode);
                        updateStatus('ready');
                    } else {
                        console.error("[AudioWS] Handshake failed:", response.message);
                        updateStatus('error');
                        onErrorRef.current?.(new Error(response.message || 'Handshake failed'));
                    }
                } catch (e) {
                    console.error("[AudioWS] Failed to parse handshake response:", e);
                }
                return;
            }

            // Îç∞Ïù¥ÌÑ∞ ÏùëÎãµ
            if (typeof event.data === 'string') {
                // JSON ÏùëÎãµ (transcript)
                try {
                    console.log("[AudioWS] Received text message:", event.data);
                    const data: TranscriptMessage = JSON.parse(event.data);
                    console.log("[AudioWS] Parsed JSON:", {
                        type: data.type,
                        typeCheck: data.type === 'transcript',
                        original: data.original,
                        translated: data.translated,
                        text: data.text,
                        isFinal: data.isFinal,
                    });
                    if (data.type === 'transcript') {
                        console.log("[AudioWS] Type check passed! Calling onTranscriptRef.current");
                        console.log("[AudioWS] onTranscriptRef.current exists:", !!onTranscriptRef.current);
                        const transcriptData: TranscriptData = {
                            participantId: data.participantId || participantIdRef.current,
                            original: data.original || data.text,
                            translated: data.translated || data.text,
                            isFinal: data.isFinal,
                        };
                        console.log("[AudioWS] Calling callback with:", transcriptData);
                        onTranscriptRef.current?.(transcriptData);
                        console.log("[AudioWS] Callback invoked");
                    } else {
                        console.log("[AudioWS] Type check failed! data.type =", data.type);
                    }
                } catch (e) {
                    console.error("[AudioWS] Failed to parse transcript message:", e);
                }
            } else if (event.data instanceof ArrayBuffer) {
                // Binary ÏùëÎãµ (TTS audio - MP3 format)
                console.log("[AudioWS] Received audio data:", event.data.byteLength, "bytes", "participantId:", participantIdRef.current);
                onAudioRef.current?.(event.data, 24000, participantIdRef.current);
            }
        };

        ws.onclose = (event) => {
            console.log("[AudioWS] Disconnected, code:", event.code, "reason:", event.reason);
            if (!isMountedRef.current) return;

            updateStatus('disconnected');
            isHandshakeCompleteRef.current = false;
            setSessionId(null);
            setMode(null);

            // Ïû¨Ïó∞Í≤∞ ÏãúÎèÑ (3Ï¥à ÌõÑ) - mounted ÏÉÅÌÉúÏù¥Í≥† enabledÏùº ÎïåÎßå
            if (isMountedRef.current && enabledRef.current) {
                reconnectTimeoutRef.current = setTimeout(() => {
                    if (isMountedRef.current && enabledRef.current) {
                        connect();
                    }
                }, 3000);
            }
        };

        ws.onerror = (event) => {
            console.error("[AudioWS] Error:", event);
            if (isMountedRef.current) {
                onErrorRef.current?.(new Error('WebSocket connection error'));
            }
        };
    }, [sampleRate, channels, bitsPerSample, updateStatus]);

    const disconnect = useCallback(() => {
        if (reconnectTimeoutRef.current) {
            clearTimeout(reconnectTimeoutRef.current);
            reconnectTimeoutRef.current = null;
        }

        if (wsRef.current) {
            wsRef.current.close();
            wsRef.current = null;
        }

        isHandshakeCompleteRef.current = false;
        setSessionId(null);
        setMode(null);
        updateStatus('disconnected');
    }, [updateStatus]);

    const sendAudio = useCallback((audioData: ArrayBuffer | Int16Array) => {
        if (wsRef.current?.readyState !== WebSocket.OPEN || !isHandshakeCompleteRef.current) {
            return false;
        }

        try {
            if (audioData instanceof Int16Array) {
                wsRef.current.send(audioData.buffer);
            } else {
                wsRef.current.send(audioData);
            }
            return true;
        } catch (e) {
            console.error("[AudioWS] Failed to send audio:", e);
            return false;
        }
    }, []);

    // Mount/Unmount Ï∂îÏ†Å
    useEffect(() => {
        isMountedRef.current = true;
        return () => {
            isMountedRef.current = false;
        };
    }, []);

    // enabled Î≥ÄÍ≤Ω Ïãú Ïó∞Í≤∞/Ìï¥Ï†ú
    useEffect(() => {
        if (enabled) {
            // ÏïΩÍ∞ÑÏùò ÏßÄÏó∞ÏùÑ ÎëêÏñ¥ React strict modeÏùò Ïù¥Ï§ë ÎßàÏö¥Ìä∏ Ï≤òÎ¶¨
            const timer = setTimeout(() => {
                if (isMountedRef.current && enabledRef.current) {
                    connect();
                }
            }, 100);
            return () => clearTimeout(timer);
        } else {
            disconnect();
        }
    }, [enabled]); // connect, disconnectÎ•º ÏùòÏ°¥ÏÑ±ÏóêÏÑú Ï†úÏô∏ (ÏïàÏ†ïÏ†ÅÏù∏ Ï∞∏Ï°∞)

    // ÏµúÏ¢Ö ÌÅ¥Î¶∞ÏóÖ
    useEffect(() => {
        return () => {
            if (reconnectTimeoutRef.current) {
                clearTimeout(reconnectTimeoutRef.current);
            }
            if (wsRef.current) {
                wsRef.current.close();
                wsRef.current = null;
            }
        };
    }, []);

    return {
        status,
        sessionId,
        mode,
        isConnected: status === 'ready',
        sendAudio,
        reconnect: connect,
        disconnect,
    };
}
